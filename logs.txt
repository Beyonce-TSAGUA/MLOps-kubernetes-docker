
==> Audit <==
|--------------|-----------------------------|----------|---------------------|---------|---------------------|---------------------|
|   Command    |            Args             | Profile  |        User         | Version |     Start Time      |      End Time       |
|--------------|-----------------------------|----------|---------------------|---------|---------------------|---------------------|
| start        | --driver=hyperv             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 28 Jan 25 21:16 CET | 28 Jan 25 21:20 CET |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 28 Jan 25 21:32 CET | 28 Jan 25 21:33 CET |
| dashboard    |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 28 Jan 25 21:35 CET |                     |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 28 Jan 25 21:37 CET |                     |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 28 Jan 25 21:46 CET |                     |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 04 Feb 25 21:16 CET |                     |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 04 Feb 25 21:20 CET |                     |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 04 Feb 25 22:59 CET |                     |
| start        | --driver=hyperv             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 05 Feb 25 12:32 CET |                     |
| start        | --driver=hyperv             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 05 Feb 25 12:33 CET |                     |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 05 Feb 25 12:41 CET |                     |
| update-check |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 05 Feb 25 12:49 CET | 05 Feb 25 12:49 CET |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 05 Feb 25 12:52 CET |                     |
| delete       |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 05 Feb 25 12:53 CET | 05 Feb 25 12:53 CET |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 05 Feb 25 12:53 CET | 05 Feb 25 12:58 CET |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 07 Feb 25 16:37 CET |                     |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 07 Feb 25 16:42 CET |                     |
| config       | set driver docker           | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 07 Feb 25 16:50 CET | 07 Feb 25 16:50 CET |
| config       | set driver docker           | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 07 Feb 25 16:50 CET | 07 Feb 25 16:50 CET |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 07 Feb 25 16:50 CET |                     |
| delete       |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 07 Feb 25 17:03 CET | 07 Feb 25 17:03 CET |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 07 Feb 25 17:03 CET | 07 Feb 25 17:04 CET |
| service      | todo-app --url              | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 07 Feb 25 17:25 CET |                     |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 08 Feb 25 13:07 CET |                     |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 08 Feb 25 13:23 CET |                     |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 08 Feb 25 13:37 CET |                     |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 16 Feb 25 14:00 CET |                     |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 17 Feb 25 05:06 CET |                     |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 17 Feb 25 05:22 CET |                     |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 19 Feb 25 08:55 CET |                     |
| addons       | disable storage-provisioner | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 19 Feb 25 09:08 CET |                     |
| update-check |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 19 Feb 25 09:21 CET | 19 Feb 25 09:21 CET |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 27 Feb 25 16:27 CET |                     |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 05 Mar 25 12:33 CET |                     |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 05 Mar 25 12:34 CET |                     |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 10 Mar 25 16:55 CET |                     |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 10 Mar 25 17:04 CET |                     |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 18:55 CET | 13 Mar 25 18:57 CET |
| service      | --all -n vote-kubernetes    | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 19:43 CET | 13 Mar 25 19:44 CET |
| start        | --driver=docker             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 19:49 CET | 13 Mar 25 19:50 CET |
| service      | --all -n vote-kubernetes    | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 19:51 CET | 13 Mar 25 19:59 CET |
| service      | --all -n vote-kubernetes    | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 20:15 CET | 13 Mar 25 20:23 CET |
| service      | --all -n vote-kubernetes    | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 20:23 CET | 13 Mar 25 20:26 CET |
| service      | --all -n vote-kubernetes    | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 20:30 CET | 13 Mar 25 20:31 CET |
| delete       |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 20:43 CET | 13 Mar 25 20:43 CET |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 20:43 CET | 13 Mar 25 20:44 CET |
| service      | --all -n vote-kubernetes    | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 20:45 CET |                     |
| start        |                             | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 20:46 CET | 13 Mar 25 20:48 CET |
| service      | --all -n vote-kubernetes    | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 20:48 CET |                     |
| service      | --all -n vote-kubernetes    | minikube | DESKTOP-BEYONCE\GLC | v1.35.0 | 13 Mar 25 20:56 CET |                     |
|--------------|-----------------------------|----------|---------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/03/13 20:46:38
Running on machine: DESKTOP-Beyonce
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0313 20:46:38.360152   24992 out.go:345] Setting OutFile to fd 88 ...
I0313 20:46:38.395619   24992 out.go:397] isatty.IsTerminal(88) = true
I0313 20:46:38.395619   24992 out.go:358] Setting ErrFile to fd 100...
I0313 20:46:38.395619   24992 out.go:397] isatty.IsTerminal(100) = true
I0313 20:46:38.426555   24992 out.go:352] Setting JSON to false
I0313 20:46:38.434675   24992 start.go:129] hostinfo: {"hostname":"DESKTOP-Beyonce","uptime":223342,"bootTime":1741671856,"procs":381,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.22631.4460 Build 22631.4460","kernelVersion":"10.0.22631.4460 Build 22631.4460","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"a9789e31-21ff-4fa2-a3f2-eb357d8ca4ee"}
W0313 20:46:38.434675   24992 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0313 20:46:38.437466   24992 out.go:177] ðŸ˜„  minikube v1.35.0 on Microsoft Windows 11 Pro 10.0.22631.4460 Build 22631.4460
I0313 20:46:38.438813   24992 notify.go:220] Checking for updates...
I0313 20:46:38.440463   24992 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0313 20:46:38.442882   24992 driver.go:394] Setting default libvirt URI to qemu:///system
I0313 20:46:38.581564   24992 docker.go:123] docker version: linux-27.5.1:Docker Desktop 4.38.0 (181591)
I0313 20:46:38.585120   24992 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0313 20:46:38.927056   24992 info.go:266] docker info: {ID:0edc5d7e-bdf8-4aba-8233-c2627687b3c2 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:56 OomKillDisable:true NGoroutines:87 SystemTime:2025-03-13 19:46:38.909783546 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8248123392 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0313 20:46:38.931368   24992 out.go:177] âœ¨  Using the docker driver based on existing profile
I0313 20:46:38.932459   24992 start.go:297] selected driver: docker
I0313 20:46:38.932459   24992 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\GLC:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0313 20:46:38.932459   24992 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0313 20:46:38.940594   24992 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0313 20:46:39.328972   24992 info.go:266] docker info: {ID:0edc5d7e-bdf8-4aba-8233-c2627687b3c2 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:0 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:56 OomKillDisable:true NGoroutines:87 SystemTime:2025-03-13 19:46:39.307821753 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8248123392 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0313 20:46:39.431809   24992 cni.go:84] Creating CNI manager for ""
I0313 20:46:39.431809   24992 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0313 20:46:39.431809   24992 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\GLC:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0313 20:46:39.433949   24992 out.go:177] ðŸ‘  Starting "minikube" primary control-plane node in "minikube" cluster
I0313 20:46:39.435149   24992 cache.go:121] Beginning downloading kic base image for docker with docker
I0313 20:46:39.436188   24992 out.go:177] ðŸšœ  Pulling base image v0.0.46 ...
I0313 20:46:39.437814   24992 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0313 20:46:39.437814   24992 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0313 20:46:39.437814   24992 preload.go:146] Found local preload: C:\Users\GLC\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0313 20:46:39.437814   24992 cache.go:56] Caching tarball of preloaded images
I0313 20:46:39.438377   24992 preload.go:172] Found C:\Users\GLC\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0313 20:46:39.438601   24992 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0313 20:46:39.438601   24992 profile.go:143] Saving config to C:\Users\GLC\.minikube\profiles\minikube\config.json ...
I0313 20:46:39.519555   24992 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0313 20:46:39.520096   24992 localpath.go:146] windows sanitize: C:\Users\GLC\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\GLC\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0313 20:46:39.520096   24992 localpath.go:146] windows sanitize: C:\Users\GLC\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\GLC\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0313 20:46:39.520096   24992 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0313 20:46:39.521138   24992 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory, skipping pull
I0313 20:46:39.521138   24992 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in cache, skipping pull
I0313 20:46:39.521673   24992 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0313 20:46:39.521673   24992 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0313 20:46:39.521673   24992 localpath.go:146] windows sanitize: C:\Users\GLC\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\GLC\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0313 20:47:33.752248   24992 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0313 20:47:33.752248   24992 cache.go:227] Successfully downloaded all kic artifacts
I0313 20:47:33.752817   24992 start.go:360] acquireMachinesLock for minikube: {Name:mk14851947cce8155d2401849ee6db0ec0f69be5 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0313 20:47:33.752817   24992 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0313 20:47:33.752817   24992 start.go:96] Skipping create...Using existing machine configuration
I0313 20:47:33.752817   24992 fix.go:54] fixHost starting: 
I0313 20:47:33.761813   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:47:33.825771   24992 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0313 20:47:33.825771   24992 fix.go:112] recreateIfNeeded on minikube: state= err=unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:33.825771   24992 fix.go:117] machineExists: false. err=machine does not exist
I0313 20:47:33.827116   24992 out.go:177] ðŸ¤·  docker "minikube" container is missing, will recreate.
I0313 20:47:33.829891   24992 delete.go:124] DEMOLISHING minikube ...
I0313 20:47:33.838561   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:47:33.909884   24992 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0313 20:47:33.909884   24992 stop.go:83] unable to get state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:33.910425   24992 delete.go:128] stophost failed (probably ok): ssh power off: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:33.919452   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:47:33.990063   24992 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0313 20:47:33.990063   24992 delete.go:82] Unable to get host status for minikube, assuming it has already been deleted: state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:33.994562   24992 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0313 20:47:34.071584   24992 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0313 20:47:34.071584   24992 kic.go:371] could not find the container minikube to remove it. will try anyways
I0313 20:47:34.076305   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:47:34.143988   24992 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0313 20:47:34.143988   24992 oci.go:84] error getting container status, will try to delete anyways: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:34.148067   24992 cli_runner.go:164] Run: docker exec --privileged -t minikube /bin/bash -c "sudo init 0"
W0313 20:47:34.216354   24992 cli_runner.go:211] docker exec --privileged -t minikube /bin/bash -c "sudo init 0" returned with exit code 1
I0313 20:47:34.216354   24992 oci.go:656] error shutdown minikube: docker exec --privileged -t minikube /bin/bash -c "sudo init 0": exit status 1
stdout:

stderr:
Error response from daemon: No such container: minikube
I0313 20:47:35.221588   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:47:35.301245   24992 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0313 20:47:35.301245   24992 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:35.301245   24992 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0313 20:47:35.301245   24992 retry.go:31] will retry after 677.244329ms: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:35.984434   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:47:36.051265   24992 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0313 20:47:36.051265   24992 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:36.051265   24992 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0313 20:47:36.051265   24992 retry.go:31] will retry after 1.019144453s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:37.074162   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:47:37.138838   24992 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0313 20:47:37.138838   24992 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:37.138838   24992 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0313 20:47:37.138838   24992 retry.go:31] will retry after 1.393026976s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:38.535789   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:47:38.599716   24992 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0313 20:47:38.600258   24992 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:38.600258   24992 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0313 20:47:38.600258   24992 retry.go:31] will retry after 863.898405ms: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:39.468413   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:47:39.538909   24992 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0313 20:47:39.538909   24992 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:39.538909   24992 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0313 20:47:39.538909   24992 retry.go:31] will retry after 2.147920756s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:41.692424   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:47:41.770157   24992 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0313 20:47:41.770157   24992 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:41.770157   24992 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0313 20:47:41.770157   24992 retry.go:31] will retry after 3.879609285s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:45.654005   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:47:45.727759   24992 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0313 20:47:45.727759   24992 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:45.727759   24992 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0313 20:47:45.727759   24992 retry.go:31] will retry after 3.238675551s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:48.970523   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:47:49.032425   24992 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0313 20:47:49.032425   24992 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0313 20:47:49.032425   24992 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0313 20:47:49.032425   24992 oci.go:88] couldn't shut down minikube (might be okay): verify shutdown: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
 
I0313 20:47:49.036174   24992 cli_runner.go:164] Run: docker rm -f -v minikube
I0313 20:47:49.104199   24992 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0313 20:47:49.167627   24992 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0313 20:47:49.171472   24992 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0313 20:47:49.241890   24992 cli_runner.go:164] Run: docker network rm minikube
I0313 20:47:50.139354   24992 fix.go:124] Sleeping 1 second for extra luck!
I0313 20:47:51.140079   24992 start.go:125] createHost starting for "" (driver="docker")
I0313 20:47:51.141518   24992 out.go:235] ðŸ”¥  Creating docker container (CPUs=2, Memory=4000MB) ...
I0313 20:47:51.142567   24992 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0313 20:47:51.142567   24992 client.go:168] LocalClient.Create starting
I0313 20:47:51.143119   24992 main.go:141] libmachine: Reading certificate data from C:\Users\GLC\.minikube\certs\ca.pem
I0313 20:47:51.143119   24992 main.go:141] libmachine: Decoding PEM data...
I0313 20:47:51.143119   24992 main.go:141] libmachine: Parsing certificate...
I0313 20:47:51.143119   24992 main.go:141] libmachine: Reading certificate data from C:\Users\GLC\.minikube\certs\cert.pem
I0313 20:47:51.143828   24992 main.go:141] libmachine: Decoding PEM data...
I0313 20:47:51.143828   24992 main.go:141] libmachine: Parsing certificate...
I0313 20:47:51.150113   24992 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0313 20:47:51.214063   24992 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0313 20:47:51.217498   24992 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0313 20:47:51.218051   24992 cli_runner.go:164] Run: docker network inspect minikube
W0313 20:47:51.282905   24992 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0313 20:47:51.282905   24992 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0313 20:47:51.282905   24992 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0313 20:47:51.287326   24992 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0313 20:47:51.482597   24992 network.go:206] using free private subnet 192.168.49.0/24: &{IP:192.168.49.0 Netmask:255.255.255.0 Prefix:24 CIDR:192.168.49.0/24 Gateway:192.168.49.1 ClientMin:192.168.49.2 ClientMax:192.168.49.254 Broadcast:192.168.49.255 IsPrivate:true Interface:{IfaceName: IfaceIPv4: IfaceMTU:0 IfaceMAC:} reservation:0xc0021bc420}
I0313 20:47:51.482851   24992 network_create.go:124] attempt to create docker network minikube 192.168.49.0/24 with gateway 192.168.49.1 and MTU of 1500 ...
I0313 20:47:51.486279   24992 cli_runner.go:164] Run: docker network create --driver=bridge --subnet=192.168.49.0/24 --gateway=192.168.49.1 -o --ip-masq -o --icc -o com.docker.network.driver.mtu=1500 --label=created_by.minikube.sigs.k8s.io=true --label=name.minikube.sigs.k8s.io=minikube minikube
I0313 20:47:51.859482   24992 network_create.go:108] docker network minikube 192.168.49.0/24 created
I0313 20:47:51.860013   24992 kic.go:121] calculated static IP "192.168.49.2" for the "minikube" container
I0313 20:47:51.874228   24992 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0313 20:47:51.953749   24992 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0313 20:47:52.039875   24992 oci.go:103] Successfully created a docker volume minikube
I0313 20:47:52.045465   24992 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib
I0313 20:47:53.473985   24992 cli_runner.go:217] Completed: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -d /var/lib: (1.4285202s)
I0313 20:47:53.473985   24992 oci.go:107] Successfully prepared a docker volume minikube
I0313 20:47:53.473985   24992 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0313 20:47:53.473985   24992 kic.go:194] Starting extracting preloaded images to volume ...
I0313 20:47:53.482232   24992 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\GLC\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir
I0313 20:48:05.354008   24992 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\GLC\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 -I lz4 -xf /preloaded.tar -C /extractDir: (11.8717762s)
I0313 20:48:05.354008   24992 kic.go:203] duration metric: took 11.8800236s to extract preloaded images to volume ...
I0313 20:48:05.358765   24992 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0313 20:48:05.709191   24992 info.go:266] docker info: {ID:0edc5d7e-bdf8-4aba-8233-c2627687b3c2 Containers:0 ContainersRunning:0 ContainersPaused:0 ContainersStopped:0 Images:1 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:60 OomKillDisable:true NGoroutines:93 SystemTime:2025-03-13 19:48:05.696035354 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:20 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:8248123392 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Program Files\Docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0313 20:48:05.712584   24992 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0313 20:48:06.042970   24992 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --network minikube --ip 192.168.49.2 --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279
I0313 20:48:06.626576   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0313 20:48:06.721419   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0313 20:48:06.795637   24992 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0313 20:48:06.936380   24992 oci.go:144] the created container "minikube" has a running status.
I0313 20:48:06.936380   24992 kic.go:225] Creating ssh key for kic: C:\Users\GLC\.minikube\machines\minikube\id_rsa...
I0313 20:48:07.021589   24992 kic_runner.go:191] docker (temp): C:\Users\GLC\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0313 20:48:07.125935   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0313 20:48:07.225091   24992 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0313 20:48:07.225091   24992 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0313 20:48:07.325623   24992 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\GLC\.minikube\machines\minikube\id_rsa...
I0313 20:48:07.846563   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0313 20:48:07.911075   24992 machine.go:93] provisionDockerMachine start ...
I0313 20:48:07.914489   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:07.982010   24992 main.go:141] libmachine: Using SSH client type: native
I0313 20:48:07.993068   24992 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc05360] 0xc07ea0 <nil>  [] 0s} 127.0.0.1 49680 <nil> <nil>}
I0313 20:48:07.993068   24992 main.go:141] libmachine: About to run SSH command:
hostname
I0313 20:48:08.141863   24992 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0313 20:48:08.141863   24992 ubuntu.go:169] provisioning hostname "minikube"
I0313 20:48:08.145631   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:08.206855   24992 main.go:141] libmachine: Using SSH client type: native
I0313 20:48:08.206855   24992 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc05360] 0xc07ea0 <nil>  [] 0s} 127.0.0.1 49680 <nil> <nil>}
I0313 20:48:08.207400   24992 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0313 20:48:08.374406   24992 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0313 20:48:08.378751   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:08.441874   24992 main.go:141] libmachine: Using SSH client type: native
I0313 20:48:08.442531   24992 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc05360] 0xc07ea0 <nil>  [] 0s} 127.0.0.1 49680 <nil> <nil>}
I0313 20:48:08.442531   24992 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0313 20:48:08.592449   24992 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0313 20:48:08.592449   24992 ubuntu.go:175] set auth options {CertDir:C:\Users\GLC\.minikube CaCertPath:C:\Users\GLC\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\GLC\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\GLC\.minikube\machines\server.pem ServerKeyPath:C:\Users\GLC\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\GLC\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\GLC\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\GLC\.minikube}
I0313 20:48:08.592449   24992 ubuntu.go:177] setting up certificates
I0313 20:48:08.592449   24992 provision.go:84] configureAuth start
I0313 20:48:08.595643   24992 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0313 20:48:08.654805   24992 provision.go:143] copyHostCerts
I0313 20:48:08.654805   24992 exec_runner.go:144] found C:\Users\GLC\.minikube/ca.pem, removing ...
I0313 20:48:08.654805   24992 exec_runner.go:203] rm: C:\Users\GLC\.minikube\ca.pem
I0313 20:48:08.654805   24992 exec_runner.go:151] cp: C:\Users\GLC\.minikube\certs\ca.pem --> C:\Users\GLC\.minikube/ca.pem (1070 bytes)
I0313 20:48:08.655326   24992 exec_runner.go:144] found C:\Users\GLC\.minikube/cert.pem, removing ...
I0313 20:48:08.655851   24992 exec_runner.go:203] rm: C:\Users\GLC\.minikube\cert.pem
I0313 20:48:08.655851   24992 exec_runner.go:151] cp: C:\Users\GLC\.minikube\certs\cert.pem --> C:\Users\GLC\.minikube/cert.pem (1115 bytes)
I0313 20:48:08.656377   24992 exec_runner.go:144] found C:\Users\GLC\.minikube/key.pem, removing ...
I0313 20:48:08.656377   24992 exec_runner.go:203] rm: C:\Users\GLC\.minikube\key.pem
I0313 20:48:08.656377   24992 exec_runner.go:151] cp: C:\Users\GLC\.minikube\certs\key.pem --> C:\Users\GLC\.minikube/key.pem (1675 bytes)
I0313 20:48:08.656903   24992 provision.go:117] generating server cert: C:\Users\GLC\.minikube\machines\server.pem ca-key=C:\Users\GLC\.minikube\certs\ca.pem private-key=C:\Users\GLC\.minikube\certs\ca-key.pem org=GLC.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0313 20:48:08.728545   24992 provision.go:177] copyRemoteCerts
I0313 20:48:08.737559   24992 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0313 20:48:08.741682   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:08.799716   24992 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49680 SSHKeyPath:C:\Users\GLC\.minikube\machines\minikube\id_rsa Username:docker}
I0313 20:48:08.906105   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0313 20:48:08.930945   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\machines\server.pem --> /etc/docker/server.pem (1172 bytes)
I0313 20:48:08.962909   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0313 20:48:08.993473   24992 provision.go:87] duration metric: took 401.024ms to configureAuth
I0313 20:48:08.993473   24992 ubuntu.go:193] setting minikube options for container-runtime
I0313 20:48:08.994006   24992 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0313 20:48:08.998487   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:09.056849   24992 main.go:141] libmachine: Using SSH client type: native
I0313 20:48:09.057429   24992 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc05360] 0xc07ea0 <nil>  [] 0s} 127.0.0.1 49680 <nil> <nil>}
I0313 20:48:09.057429   24992 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0313 20:48:09.201539   24992 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0313 20:48:09.201539   24992 ubuntu.go:71] root file system type: overlay
I0313 20:48:09.201539   24992 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0313 20:48:09.205330   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:09.261722   24992 main.go:141] libmachine: Using SSH client type: native
I0313 20:48:09.262289   24992 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc05360] 0xc07ea0 <nil>  [] 0s} 127.0.0.1 49680 <nil> <nil>}
I0313 20:48:09.262289   24992 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0313 20:48:09.422763   24992 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0313 20:48:09.426130   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:09.497121   24992 main.go:141] libmachine: Using SSH client type: native
I0313 20:48:09.497121   24992 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc05360] 0xc07ea0 <nil>  [] 0s} 127.0.0.1 49680 <nil> <nil>}
I0313 20:48:09.497121   24992 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0313 20:48:12.628303   24992 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-03-13 19:48:09.411984427 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0313 20:48:12.628303   24992 machine.go:96] duration metric: took 4.7172278s to provisionDockerMachine
I0313 20:48:12.628303   24992 client.go:171] duration metric: took 21.4857358s to LocalClient.Create
I0313 20:48:12.628303   24992 start.go:167] duration metric: took 21.4857358s to libmachine.API.Create "minikube"
I0313 20:48:12.628303   24992 start.go:293] postStartSetup for "minikube" (driver="docker")
I0313 20:48:12.628303   24992 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0313 20:48:12.636140   24992 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0313 20:48:12.639263   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:12.692117   24992 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49680 SSHKeyPath:C:\Users\GLC\.minikube\machines\minikube\id_rsa Username:docker}
I0313 20:48:12.805718   24992 ssh_runner.go:195] Run: cat /etc/os-release
I0313 20:48:12.810975   24992 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0313 20:48:12.810975   24992 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0313 20:48:12.810975   24992 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0313 20:48:12.810975   24992 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0313 20:48:12.810975   24992 filesync.go:126] Scanning C:\Users\GLC\.minikube\addons for local assets ...
I0313 20:48:12.811480   24992 filesync.go:126] Scanning C:\Users\GLC\.minikube\files for local assets ...
I0313 20:48:12.811480   24992 start.go:296] duration metric: took 183.1772ms for postStartSetup
I0313 20:48:12.815858   24992 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0313 20:48:12.870311   24992 profile.go:143] Saving config to C:\Users\GLC\.minikube\profiles\minikube\config.json ...
I0313 20:48:12.879487   24992 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0313 20:48:12.883987   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:12.936761   24992 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49680 SSHKeyPath:C:\Users\GLC\.minikube\machines\minikube\id_rsa Username:docker}
I0313 20:48:13.040039   24992 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0313 20:48:13.046768   24992 start.go:128] duration metric: took 21.9066889s to createHost
I0313 20:48:13.052901   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0313 20:48:13.105510   24992 fix.go:138] unexpected machine state, will restart: <nil>
I0313 20:48:13.105510   24992 machine.go:93] provisionDockerMachine start ...
I0313 20:48:13.108327   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:13.163163   24992 main.go:141] libmachine: Using SSH client type: native
I0313 20:48:13.163163   24992 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc05360] 0xc07ea0 <nil>  [] 0s} 127.0.0.1 49680 <nil> <nil>}
I0313 20:48:13.163163   24992 main.go:141] libmachine: About to run SSH command:
hostname
I0313 20:48:13.311096   24992 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0313 20:48:13.311096   24992 ubuntu.go:169] provisioning hostname "minikube"
I0313 20:48:13.314851   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:13.370159   24992 main.go:141] libmachine: Using SSH client type: native
I0313 20:48:13.370159   24992 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc05360] 0xc07ea0 <nil>  [] 0s} 127.0.0.1 49680 <nil> <nil>}
I0313 20:48:13.370159   24992 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0313 20:48:13.522978   24992 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0313 20:48:13.526765   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:13.581378   24992 main.go:141] libmachine: Using SSH client type: native
I0313 20:48:13.581947   24992 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc05360] 0xc07ea0 <nil>  [] 0s} 127.0.0.1 49680 <nil> <nil>}
I0313 20:48:13.581947   24992 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0313 20:48:13.722101   24992 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0313 20:48:13.722101   24992 ubuntu.go:175] set auth options {CertDir:C:\Users\GLC\.minikube CaCertPath:C:\Users\GLC\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\GLC\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\GLC\.minikube\machines\server.pem ServerKeyPath:C:\Users\GLC\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\GLC\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\GLC\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\GLC\.minikube}
I0313 20:48:13.722101   24992 ubuntu.go:177] setting up certificates
I0313 20:48:13.722101   24992 provision.go:84] configureAuth start
I0313 20:48:13.726465   24992 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0313 20:48:13.779544   24992 provision.go:143] copyHostCerts
I0313 20:48:13.779544   24992 exec_runner.go:144] found C:\Users\GLC\.minikube/ca.pem, removing ...
I0313 20:48:13.779544   24992 exec_runner.go:203] rm: C:\Users\GLC\.minikube\ca.pem
I0313 20:48:13.780055   24992 exec_runner.go:151] cp: C:\Users\GLC\.minikube\certs\ca.pem --> C:\Users\GLC\.minikube/ca.pem (1070 bytes)
I0313 20:48:13.780588   24992 exec_runner.go:144] found C:\Users\GLC\.minikube/cert.pem, removing ...
I0313 20:48:13.780588   24992 exec_runner.go:203] rm: C:\Users\GLC\.minikube\cert.pem
I0313 20:48:13.780588   24992 exec_runner.go:151] cp: C:\Users\GLC\.minikube\certs\cert.pem --> C:\Users\GLC\.minikube/cert.pem (1115 bytes)
I0313 20:48:13.781132   24992 exec_runner.go:144] found C:\Users\GLC\.minikube/key.pem, removing ...
I0313 20:48:13.781132   24992 exec_runner.go:203] rm: C:\Users\GLC\.minikube\key.pem
I0313 20:48:13.781681   24992 exec_runner.go:151] cp: C:\Users\GLC\.minikube\certs\key.pem --> C:\Users\GLC\.minikube/key.pem (1675 bytes)
I0313 20:48:13.782231   24992 provision.go:117] generating server cert: C:\Users\GLC\.minikube\machines\server.pem ca-key=C:\Users\GLC\.minikube\certs\ca.pem private-key=C:\Users\GLC\.minikube\certs\ca-key.pem org=GLC.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0313 20:48:13.928068   24992 provision.go:177] copyRemoteCerts
I0313 20:48:13.935605   24992 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0313 20:48:13.938560   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:13.992180   24992 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49680 SSHKeyPath:C:\Users\GLC\.minikube\machines\minikube\id_rsa Username:docker}
I0313 20:48:14.096895   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\machines\server.pem --> /etc/docker/server.pem (1172 bytes)
I0313 20:48:14.124123   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0313 20:48:14.149890   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1070 bytes)
I0313 20:48:14.174927   24992 provision.go:87] duration metric: took 452.8263ms to configureAuth
I0313 20:48:14.174927   24992 ubuntu.go:193] setting minikube options for container-runtime
I0313 20:48:14.175512   24992 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0313 20:48:14.178964   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:14.234851   24992 main.go:141] libmachine: Using SSH client type: native
I0313 20:48:14.235396   24992 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc05360] 0xc07ea0 <nil>  [] 0s} 127.0.0.1 49680 <nil> <nil>}
I0313 20:48:14.235396   24992 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0313 20:48:14.383752   24992 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0313 20:48:14.383752   24992 ubuntu.go:71] root file system type: overlay
I0313 20:48:14.383752   24992 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0313 20:48:14.388312   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:14.446927   24992 main.go:141] libmachine: Using SSH client type: native
I0313 20:48:14.447498   24992 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc05360] 0xc07ea0 <nil>  [] 0s} 127.0.0.1 49680 <nil> <nil>}
I0313 20:48:14.447498   24992 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0313 20:48:14.605395   24992 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0313 20:48:14.608875   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:14.664985   24992 main.go:141] libmachine: Using SSH client type: native
I0313 20:48:14.664985   24992 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xc05360] 0xc07ea0 <nil>  [] 0s} 127.0.0.1 49680 <nil> <nil>}
I0313 20:48:14.664985   24992 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0313 20:48:14.808184   24992 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0313 20:48:14.808184   24992 machine.go:96] duration metric: took 1.702674s to provisionDockerMachine
I0313 20:48:14.808184   24992 start.go:293] postStartSetup for "minikube" (driver="docker")
I0313 20:48:14.808184   24992 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0313 20:48:14.816207   24992 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0313 20:48:14.819793   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:14.869897   24992 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49680 SSHKeyPath:C:\Users\GLC\.minikube\machines\minikube\id_rsa Username:docker}
I0313 20:48:14.984202   24992 ssh_runner.go:195] Run: cat /etc/os-release
I0313 20:48:14.990738   24992 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0313 20:48:14.990738   24992 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0313 20:48:14.990738   24992 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0313 20:48:14.990738   24992 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0313 20:48:14.991314   24992 filesync.go:126] Scanning C:\Users\GLC\.minikube\addons for local assets ...
I0313 20:48:14.991314   24992 filesync.go:126] Scanning C:\Users\GLC\.minikube\files for local assets ...
I0313 20:48:14.991314   24992 start.go:296] duration metric: took 183.1299ms for postStartSetup
I0313 20:48:15.000090   24992 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0313 20:48:15.003654   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:15.060229   24992 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49680 SSHKeyPath:C:\Users\GLC\.minikube\machines\minikube\id_rsa Username:docker}
I0313 20:48:15.169060   24992 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0313 20:48:15.176373   24992 fix.go:56] duration metric: took 41.4235559s for fixHost
I0313 20:48:15.176373   24992 start.go:83] releasing machines lock for "minikube", held for 41.4235559s
I0313 20:48:15.179144   24992 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0313 20:48:15.233766   24992 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0313 20:48:15.237151   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:15.240383   24992 ssh_runner.go:195] Run: cat /version.json
I0313 20:48:15.244025   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:15.295045   24992 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49680 SSHKeyPath:C:\Users\GLC\.minikube\machines\minikube\id_rsa Username:docker}
I0313 20:48:15.301456   24992 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49680 SSHKeyPath:C:\Users\GLC\.minikube\machines\minikube\id_rsa Username:docker}
W0313 20:48:15.391304   24992 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0313 20:48:15.412499   24992 ssh_runner.go:195] Run: systemctl --version
I0313 20:48:15.429327   24992 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0313 20:48:15.442615   24992 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0313 20:48:15.453951   24992 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0313 20:48:15.460968   24992 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0313 20:48:15.495313   24992 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0313 20:48:15.495313   24992 start.go:495] detecting cgroup driver to use...
I0313 20:48:15.495313   24992 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0313 20:48:15.495313   24992 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0313 20:48:15.528646   24992 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0313 20:48:15.562981   24992 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0313 20:48:15.583936   24992 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0313 20:48:15.595703   24992 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0313 20:48:15.625284   24992 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0313 20:48:15.655384   24992 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0313 20:48:15.689200   24992 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0313 20:48:15.719261   24992 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0313 20:48:15.747838   24992 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0313 20:48:15.778162   24992 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0313 20:48:15.805274   24992 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0313 20:48:15.832823   24992 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0313 20:48:15.856192   24992 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0313 20:48:15.882089   24992 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 20:48:16.010613   24992 ssh_runner.go:195] Run: sudo systemctl restart containerd
W0313 20:48:16.045495   24992 out.go:270] â—  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0313 20:48:16.046084   24992 out.go:270] ðŸ’¡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0313 20:48:16.193743   24992 start.go:495] detecting cgroup driver to use...
I0313 20:48:16.193743   24992 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0313 20:48:16.205582   24992 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0313 20:48:16.227335   24992 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0313 20:48:16.239940   24992 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0313 20:48:16.259863   24992 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0313 20:48:16.299079   24992 ssh_runner.go:195] Run: which cri-dockerd
I0313 20:48:16.319288   24992 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0313 20:48:16.335643   24992 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0313 20:48:16.371835   24992 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0313 20:48:16.513173   24992 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0313 20:48:16.634311   24992 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0313 20:48:16.634881   24992 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0313 20:48:16.674080   24992 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 20:48:16.845321   24992 ssh_runner.go:195] Run: sudo systemctl restart docker
I0313 20:48:20.853487   24992 ssh_runner.go:235] Completed: sudo systemctl restart docker: (4.0081657s)
I0313 20:48:20.862865   24992 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0313 20:48:20.893408   24992 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0313 20:48:20.921132   24992 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0313 20:48:21.061599   24992 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0313 20:48:21.189909   24992 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 20:48:21.308938   24992 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0313 20:48:21.340972   24992 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0313 20:48:21.370289   24992 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 20:48:21.502847   24992 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0313 20:48:21.610426   24992 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0313 20:48:21.621935   24992 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0313 20:48:21.630979   24992 start.go:563] Will wait 60s for crictl version
I0313 20:48:21.642573   24992 ssh_runner.go:195] Run: which crictl
I0313 20:48:21.662040   24992 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0313 20:48:21.711983   24992 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0313 20:48:21.717035   24992 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0313 20:48:21.758729   24992 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0313 20:48:21.797797   24992 out.go:235] ðŸ³  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0313 20:48:21.802895   24992 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0313 20:48:21.951239   24992 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0313 20:48:21.960048   24992 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0313 20:48:21.969490   24992 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0313 20:48:21.992855   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0313 20:48:22.067838   24992 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\GLC:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0313 20:48:22.067838   24992 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0313 20:48:22.071867   24992 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0313 20:48:22.101774   24992 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0313 20:48:22.101774   24992 docker.go:619] Images already preloaded, skipping extraction
I0313 20:48:22.106289   24992 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0313 20:48:22.134250   24992 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0313 20:48:22.134250   24992 cache_images.go:84] Images are preloaded, skipping loading
I0313 20:48:22.134250   24992 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0313 20:48:22.134250   24992 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0313 20:48:22.138494   24992 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0313 20:48:22.200840   24992 cni.go:84] Creating CNI manager for ""
I0313 20:48:22.200840   24992 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0313 20:48:22.200840   24992 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0313 20:48:22.200840   24992 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0313 20:48:22.200840   24992 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0313 20:48:22.210114   24992 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0313 20:48:22.225182   24992 binaries.go:44] Found k8s binaries, skipping transfer
I0313 20:48:22.236880   24992 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0313 20:48:22.253339   24992 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0313 20:48:22.283825   24992 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0313 20:48:22.315007   24992 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0313 20:48:22.357954   24992 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0313 20:48:22.366293   24992 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0313 20:48:22.398206   24992 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 20:48:22.530720   24992 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0313 20:48:22.555692   24992 certs.go:68] Setting up C:\Users\GLC\.minikube\profiles\minikube for IP: 192.168.49.2
I0313 20:48:22.555788   24992 certs.go:194] generating shared ca certs ...
I0313 20:48:22.555788   24992 certs.go:226] acquiring lock for ca certs: {Name:mk1f5d1856b411fa780d0e20bc7b98043a801df5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 20:48:22.556365   24992 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\GLC\.minikube\ca.key
I0313 20:48:22.556365   24992 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\GLC\.minikube\proxy-client-ca.key
I0313 20:48:22.556365   24992 certs.go:256] generating profile certs ...
I0313 20:48:22.556920   24992 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\GLC\.minikube\profiles\minikube\client.key
I0313 20:48:22.557539   24992 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\GLC\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0313 20:48:22.557539   24992 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\GLC\.minikube\profiles\minikube\proxy-client.key
I0313 20:48:22.558559   24992 certs.go:484] found cert: C:\Users\GLC\.minikube\certs\ca-key.pem (1679 bytes)
I0313 20:48:22.558817   24992 certs.go:484] found cert: C:\Users\GLC\.minikube\certs\ca.pem (1070 bytes)
I0313 20:48:22.558817   24992 certs.go:484] found cert: C:\Users\GLC\.minikube\certs\cert.pem (1115 bytes)
I0313 20:48:22.558817   24992 certs.go:484] found cert: C:\Users\GLC\.minikube\certs\key.pem (1675 bytes)
I0313 20:48:22.560037   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0313 20:48:22.598286   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0313 20:48:22.638973   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0313 20:48:22.676152   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I0313 20:48:22.709677   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0313 20:48:22.745452   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I0313 20:48:22.782087   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0313 20:48:22.816535   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0313 20:48:22.853119   24992 ssh_runner.go:362] scp C:\Users\GLC\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0313 20:48:22.890440   24992 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0313 20:48:22.926407   24992 ssh_runner.go:195] Run: openssl version
I0313 20:48:22.949563   24992 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0313 20:48:22.976495   24992 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0313 20:48:22.985069   24992 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jan 28 20:20 /usr/share/ca-certificates/minikubeCA.pem
I0313 20:48:22.995973   24992 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0313 20:48:23.021242   24992 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0313 20:48:23.048101   24992 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0313 20:48:23.057975   24992 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0313 20:48:23.058271   24992 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\GLC:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0313 20:48:23.064080   24992 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0313 20:48:23.102081   24992 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0313 20:48:23.126760   24992 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0313 20:48:23.144248   24992 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0313 20:48:23.188215   24992 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0313 20:48:23.209100   24992 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0313 20:48:23.209100   24992 kubeadm.go:157] found existing configuration files:

I0313 20:48:23.217857   24992 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0313 20:48:23.233315   24992 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0313 20:48:23.243487   24992 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0313 20:48:23.268579   24992 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0313 20:48:23.285865   24992 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0313 20:48:23.296473   24992 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0313 20:48:23.321829   24992 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0313 20:48:23.338124   24992 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0313 20:48:23.351033   24992 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0313 20:48:23.378293   24992 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0313 20:48:23.396624   24992 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0313 20:48:23.407709   24992 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0313 20:48:23.425063   24992 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0313 20:48:23.510315   24992 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0313 20:48:23.519352   24992 kubeadm.go:310] 	[WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
I0313 20:48:23.623269   24992 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0313 20:48:34.283539   24992 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0313 20:48:34.284045   24992 kubeadm.go:310] [preflight] Running pre-flight checks
I0313 20:48:34.284045   24992 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0313 20:48:34.284045   24992 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0313 20:48:34.284045   24992 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0313 20:48:34.284045   24992 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0313 20:48:34.285193   24992 out.go:235]     â–ª Generating certificates and keys ...
I0313 20:48:34.286258   24992 kubeadm.go:310] [certs] Using existing ca certificate authority
I0313 20:48:34.286258   24992 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0313 20:48:34.286258   24992 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0313 20:48:34.286258   24992 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0313 20:48:34.286258   24992 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0313 20:48:34.286258   24992 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0313 20:48:34.286784   24992 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0313 20:48:34.286784   24992 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0313 20:48:34.286784   24992 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0313 20:48:34.286784   24992 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [192.168.49.2 127.0.0.1 ::1]
I0313 20:48:34.286784   24992 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0313 20:48:34.286784   24992 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0313 20:48:34.287308   24992 kubeadm.go:310] [certs] Generating "sa" key and public key
I0313 20:48:34.287308   24992 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0313 20:48:34.287308   24992 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0313 20:48:34.287308   24992 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0313 20:48:34.287308   24992 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0313 20:48:34.287308   24992 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0313 20:48:34.287308   24992 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0313 20:48:34.287308   24992 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0313 20:48:34.287835   24992 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0313 20:48:34.288950   24992 out.go:235]     â–ª Booting up control plane ...
I0313 20:48:34.289476   24992 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0313 20:48:34.289476   24992 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0313 20:48:34.289476   24992 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0313 20:48:34.290028   24992 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0313 20:48:34.290028   24992 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0313 20:48:34.290028   24992 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0313 20:48:34.290028   24992 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0313 20:48:34.290582   24992 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0313 20:48:34.290582   24992 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 501.29777ms
I0313 20:48:34.290582   24992 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0313 20:48:34.290582   24992 kubeadm.go:310] [api-check] The API server is healthy after 6.002216837s
I0313 20:48:34.290582   24992 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0313 20:48:34.291105   24992 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0313 20:48:34.291105   24992 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0313 20:48:34.291105   24992 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0313 20:48:34.291105   24992 kubeadm.go:310] [bootstrap-token] Using token: 3k4b1f.r6r38n0z3ri0ib2u
I0313 20:48:34.293630   24992 out.go:235]     â–ª Configuring RBAC rules ...
I0313 20:48:34.293630   24992 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0313 20:48:34.294153   24992 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0313 20:48:34.294153   24992 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0313 20:48:34.294153   24992 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0313 20:48:34.294153   24992 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0313 20:48:34.294705   24992 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0313 20:48:34.294705   24992 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0313 20:48:34.294705   24992 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0313 20:48:34.294705   24992 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0313 20:48:34.294705   24992 kubeadm.go:310] 
I0313 20:48:34.294705   24992 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0313 20:48:34.294705   24992 kubeadm.go:310] 
I0313 20:48:34.294705   24992 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0313 20:48:34.294705   24992 kubeadm.go:310] 
I0313 20:48:34.294705   24992 kubeadm.go:310]   mkdir -p $HOME/.kube
I0313 20:48:34.294705   24992 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0313 20:48:34.294705   24992 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0313 20:48:34.294705   24992 kubeadm.go:310] 
I0313 20:48:34.294705   24992 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0313 20:48:34.294705   24992 kubeadm.go:310] 
I0313 20:48:34.295256   24992 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0313 20:48:34.295256   24992 kubeadm.go:310] 
I0313 20:48:34.295256   24992 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0313 20:48:34.295256   24992 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0313 20:48:34.295256   24992 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0313 20:48:34.295256   24992 kubeadm.go:310] 
I0313 20:48:34.295256   24992 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0313 20:48:34.295256   24992 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0313 20:48:34.295256   24992 kubeadm.go:310] 
I0313 20:48:34.295256   24992 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token 3k4b1f.r6r38n0z3ri0ib2u \
I0313 20:48:34.295256   24992 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:5fac28926fe3f537e133dcdbe6a75bf8e94ee01010b1448de967bf96b8317416 \
I0313 20:48:34.295809   24992 kubeadm.go:310] 	--control-plane 
I0313 20:48:34.295809   24992 kubeadm.go:310] 
I0313 20:48:34.295874   24992 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0313 20:48:34.295874   24992 kubeadm.go:310] 
I0313 20:48:34.295874   24992 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token 3k4b1f.r6r38n0z3ri0ib2u \
I0313 20:48:34.295874   24992 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:5fac28926fe3f537e133dcdbe6a75bf8e94ee01010b1448de967bf96b8317416 
I0313 20:48:34.295874   24992 cni.go:84] Creating CNI manager for ""
I0313 20:48:34.295874   24992 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0313 20:48:34.297144   24992 out.go:177] ðŸ”—  Configuring bridge CNI (Container Networking Interface) ...
I0313 20:48:34.306700   24992 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0313 20:48:34.321547   24992 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0313 20:48:34.345095   24992 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0313 20:48:34.356720   24992 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_03_13T20_48_34_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0313 20:48:34.358654   24992 ops.go:34] apiserver oom_adj: -16
I0313 20:48:34.361572   24992 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0313 20:48:34.499798   24992 kubeadm.go:1113] duration metric: took 154.7028ms to wait for elevateKubeSystemPrivileges
I0313 20:48:34.499798   24992 kubeadm.go:394] duration metric: took 11.4415264s to StartCluster
I0313 20:48:34.499798   24992 settings.go:142] acquiring lock: {Name:mka535caf0389794f207c82a82ab43afd1d7e6f1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 20:48:34.499798   24992 settings.go:150] Updating kubeconfig:  C:\Users\GLC\.kube\config
I0313 20:48:34.501399   24992 lock.go:35] WriteFile acquiring C:\Users\GLC\.kube\config: {Name:mk67105d06cae190ec668dda3ead786033352c4d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0313 20:48:34.502477   24992 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0313 20:48:34.502477   24992 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0313 20:48:34.502477   24992 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0313 20:48:34.502477   24992 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0313 20:48:34.502477   24992 addons.go:247] addon storage-provisioner should already be in state true
I0313 20:48:34.502477   24992 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0313 20:48:34.502477   24992 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0313 20:48:34.502477   24992 host.go:66] Checking if "minikube" exists ...
I0313 20:48:34.502477   24992 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0313 20:48:34.503033   24992 out.go:177] ðŸ”Ž  Verifying Kubernetes components...
I0313 20:48:34.516577   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0313 20:48:34.517110   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0313 20:48:34.520254   24992 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0313 20:48:34.592270   24992 out.go:177]     â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0313 20:48:34.593391   24992 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0313 20:48:34.593391   24992 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0313 20:48:34.594501   24992 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0313 20:48:34.594501   24992 addons.go:247] addon default-storageclass should already be in state true
I0313 20:48:34.595034   24992 host.go:66] Checking if "minikube" exists ...
I0313 20:48:34.598877   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:34.613582   24992 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0313 20:48:34.689237   24992 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0313 20:48:34.689237   24992 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0313 20:48:34.693600   24992 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49680 SSHKeyPath:C:\Users\GLC\.minikube\machines\minikube\id_rsa Username:docker}
I0313 20:48:34.696458   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0313 20:48:34.720771   24992 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0313 20:48:34.747465   24992 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0313 20:48:34.783504   24992 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:49680 SSHKeyPath:C:\Users\GLC\.minikube\machines\minikube\id_rsa Username:docker}
I0313 20:48:34.826718   24992 api_server.go:52] waiting for apiserver process to appear ...
I0313 20:48:34.837365   24992 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0313 20:48:34.853629   24992 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0313 20:48:34.856206   24992 api_server.go:72] duration metric: took 353.7285ms to wait for apiserver process to appear ...
I0313 20:48:34.856206   24992 api_server.go:88] waiting for apiserver healthz status ...
I0313 20:48:34.856206   24992 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:49684/healthz ...
I0313 20:48:34.866792   24992 api_server.go:279] https://127.0.0.1:49684/healthz returned 200:
ok
I0313 20:48:34.877163   24992 api_server.go:141] control plane version: v1.32.0
I0313 20:48:34.877719   24992 api_server.go:131] duration metric: took 21.5137ms to wait for apiserver health ...
I0313 20:48:34.877719   24992 system_pods.go:43] waiting for kube-system pods to appear ...
I0313 20:48:34.890342   24992 system_pods.go:59] 4 kube-system pods found
I0313 20:48:34.890342   24992 system_pods.go:61] "etcd-minikube" [3b9ab033-d4e3-48b4-aed5-77e99a5d141a] Running
I0313 20:48:34.890342   24992 system_pods.go:61] "kube-apiserver-minikube" [2ad0c3ea-bb55-4b50-91c6-362efdebd4bf] Running
I0313 20:48:34.890342   24992 system_pods.go:61] "kube-controller-manager-minikube" [017904af-7994-413d-be13-c77950fea490] Running
I0313 20:48:34.890342   24992 system_pods.go:61] "kube-scheduler-minikube" [8680e33f-f763-4ada-9de4-efdb5c0b0d3a] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0313 20:48:34.890342   24992 system_pods.go:74] duration metric: took 12.6222ms to wait for pod list to return data ...
I0313 20:48:34.890342   24992 kubeadm.go:582] duration metric: took 387.8644ms to wait for: map[apiserver:true system_pods:true]
I0313 20:48:34.890342   24992 node_conditions.go:102] verifying NodePressure condition ...
I0313 20:48:34.895541   24992 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0313 20:48:34.895541   24992 node_conditions.go:123] node cpu capacity is 8
I0313 20:48:34.895541   24992 node_conditions.go:105] duration metric: took 5.1993ms to run NodePressure ...
I0313 20:48:34.895541   24992 start.go:241] waiting for startup goroutines ...
I0313 20:48:34.933939   24992 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0313 20:48:35.322555   24992 out.go:177] ðŸŒŸ  Enabled addons: storage-provisioner, default-storageclass
I0313 20:48:35.323992   24992 addons.go:514] duration metric: took 821.5144ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0313 20:48:35.323992   24992 start.go:246] waiting for cluster config update ...
I0313 20:48:35.323992   24992 start.go:255] writing updated cluster config ...
I0313 20:48:35.332886   24992 ssh_runner.go:195] Run: rm -f paused
I0313 20:48:35.461476   24992 start.go:600] kubectl: 1.32.0, cluster: 1.32.0 (minor skew: 0)
I0313 20:48:35.462571   24992 out.go:177] ðŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Mar 13 19:48:27 minikube cri-dockerd[1756]: time="2025-03-13T19:48:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a64d1264bbb43925ae606160faf699e28d4e3c43cb946094642ea8aba2c0532f/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Mar 13 19:48:27 minikube cri-dockerd[1756]: time="2025-03-13T19:48:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b4fe030f0f087a0c13bcf738634927bbdc12f2198fb7f2641294d5f96f47c7f7/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Mar 13 19:48:27 minikube cri-dockerd[1756]: time="2025-03-13T19:48:27Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e997ed9cb01aa4df26b8ff99b603aeb480865163ea248472fa57bb0cfb86415b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Mar 13 19:48:38 minikube cri-dockerd[1756]: time="2025-03-13T19:48:38Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a35980f7197921f4c2cffcb556da4e2f4a795d02d7dd1e8f220ecff1f3998e74/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Mar 13 19:48:39 minikube cri-dockerd[1756]: time="2025-03-13T19:48:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/97813a73684121912ab35cfcf31c8360f71a8a07be517d1dd9c99ee73abedc5b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Mar 13 19:48:39 minikube cri-dockerd[1756]: time="2025-03-13T19:48:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/db3852b36d3f4acbdde69f44069b2f71ce0096c96dc34916b148ae1ba3225cc2/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Mar 13 19:48:39 minikube cri-dockerd[1756]: time="2025-03-13T19:48:39Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d0b61a35b52c83269db7665a3f7532e110ea3f76e7d8dfac7c4cb3dc484c4609/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
Mar 13 19:48:42 minikube cri-dockerd[1756]: time="2025-03-13T19:48:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9520191ba2480a0abd6f64e09d7c1471a357a3aa972a87f74e5a9f90ca0148c0/resolv.conf as [nameserver 10.96.0.10 search vote-kubernetes.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 13 19:48:42 minikube cri-dockerd[1756]: time="2025-03-13T19:48:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/f95e607ec9dfbf50c3f5d83aeb184efec597f7db017fdb5c398389e1f99b9580/resolv.conf as [nameserver 10.96.0.10 search vote-kubernetes.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 13 19:48:42 minikube cri-dockerd[1756]: time="2025-03-13T19:48:42Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/21c202672605607835855d0d729af68afeb48ea27e462c0e7e05abe8c4df99b1/resolv.conf as [nameserver 10.96.0.10 search vote-kubernetes.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 13 19:48:43 minikube cri-dockerd[1756]: time="2025-03-13T19:48:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/e649ae3c66d3cd47555d169eb4c8ce1b251576a6e53920a45c910c924bc6a6c2/resolv.conf as [nameserver 10.96.0.10 search vote-kubernetes.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 13 19:48:44 minikube cri-dockerd[1756]: time="2025-03-13T19:48:44Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
Mar 13 19:48:58 minikube cri-dockerd[1756]: time="2025-03-13T19:48:58Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/fe49169f7b543430d63629db6563b4c0f34a457c6cb86c07b6633ad5286a70e4/resolv.conf as [nameserver 10.96.0.10 search vote-kubernetes.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Mar 13 19:48:59 minikube cri-dockerd[1756]: time="2025-03-13T19:48:59Z" level=info msg="Pulling image redis:alpine: 4d03bd351ab9: Downloading [=======================>                           ]    462kB/1.003MB"
Mar 13 19:49:09 minikube cri-dockerd[1756]: time="2025-03-13T19:49:09Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [===============>                                   ]  3.951MB/12.4MB"
Mar 13 19:49:19 minikube cri-dockerd[1756]: time="2025-03-13T19:49:19Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [==========================>                        ]   6.58MB/12.4MB"
Mar 13 19:49:29 minikube cri-dockerd[1756]: time="2025-03-13T19:49:29Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [==============================>                    ]  7.514MB/12.4MB"
Mar 13 19:49:39 minikube cri-dockerd[1756]: time="2025-03-13T19:49:39Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [===============================>                   ]  7.776MB/12.4MB"
Mar 13 19:49:49 minikube cri-dockerd[1756]: time="2025-03-13T19:49:49Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [===============================>                   ]  7.912MB/12.4MB"
Mar 13 19:49:59 minikube cri-dockerd[1756]: time="2025-03-13T19:49:59Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [================================>                  ]  8.043MB/12.4MB"
Mar 13 19:50:09 minikube cri-dockerd[1756]: time="2025-03-13T19:50:09Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [=================================>                 ]  8.305MB/12.4MB"
Mar 13 19:50:19 minikube cri-dockerd[1756]: time="2025-03-13T19:50:19Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [==================================>                ]  8.436MB/12.4MB"
Mar 13 19:50:29 minikube cri-dockerd[1756]: time="2025-03-13T19:50:29Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [==================================>                ]  8.563MB/12.4MB"
Mar 13 19:50:39 minikube cri-dockerd[1756]: time="2025-03-13T19:50:39Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [===================================>               ]  8.821MB/12.4MB"
Mar 13 19:50:49 minikube cri-dockerd[1756]: time="2025-03-13T19:50:49Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [===================================>               ]  8.821MB/12.4MB"
Mar 13 19:50:59 minikube cri-dockerd[1756]: time="2025-03-13T19:50:59Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [====================================>              ]  8.952MB/12.4MB"
Mar 13 19:51:09 minikube cri-dockerd[1756]: time="2025-03-13T19:51:09Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [=====================================>             ]  9.218MB/12.4MB"
Mar 13 19:51:19 minikube cri-dockerd[1756]: time="2025-03-13T19:51:19Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [=====================================>             ]  9.349MB/12.4MB"
Mar 13 19:51:29 minikube cri-dockerd[1756]: time="2025-03-13T19:51:29Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [======================================>            ]   9.48MB/12.4MB"
Mar 13 19:51:39 minikube cri-dockerd[1756]: time="2025-03-13T19:51:39Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [=======================================>           ]  9.743MB/12.4MB"
Mar 13 19:51:49 minikube cri-dockerd[1756]: time="2025-03-13T19:51:49Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [=======================================>           ]  9.874MB/12.4MB"
Mar 13 19:51:59 minikube cri-dockerd[1756]: time="2025-03-13T19:51:59Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [========================================>          ]  10.14MB/12.4MB"
Mar 13 19:52:09 minikube cri-dockerd[1756]: time="2025-03-13T19:52:09Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [===========================================>       ]  10.67MB/12.4MB"
Mar 13 19:52:19 minikube cri-dockerd[1756]: time="2025-03-13T19:52:19Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [============================================>      ]  10.94MB/12.4MB"
Mar 13 19:52:29 minikube cri-dockerd[1756]: time="2025-03-13T19:52:29Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [============================================>      ]  11.07MB/12.4MB"
Mar 13 19:52:39 minikube cri-dockerd[1756]: time="2025-03-13T19:52:39Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [==============================================>    ]  11.59MB/12.4MB"
Mar 13 19:52:49 minikube cri-dockerd[1756]: time="2025-03-13T19:52:49Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [================================================>  ]  11.99MB/12.4MB"
Mar 13 19:52:59 minikube cri-dockerd[1756]: time="2025-03-13T19:52:59Z" level=info msg="Pulling image redis:alpine: 4738865c5f32: Downloading [=================================================> ]  12.25MB/12.4MB"
Mar 13 19:53:00 minikube cri-dockerd[1756]: time="2025-03-13T19:53:00Z" level=info msg="Stop pulling image redis:alpine: Status: Downloaded newer image for redis:alpine"
Mar 13 19:53:15 minikube cri-dockerd[1756]: time="2025-03-13T19:53:15Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 23663f1b1336: Downloading [=================>                                 ]    585kB/1.708MB"
Mar 13 19:53:25 minikube cri-dockerd[1756]: time="2025-03-13T19:53:25Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 643a6ed41aef: Downloading [===>                                               ]  2.345MB/38.18MB"
Mar 13 19:53:35 minikube cri-dockerd[1756]: time="2025-03-13T19:53:35Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 0930fdcad266: Downloading [=======================>                           ]  2.353MB/4.946MB"
Mar 13 19:53:45 minikube cri-dockerd[1756]: time="2025-03-13T19:53:45Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 0930fdcad266: Downloading [=======================================>           ]  3.861MB/4.946MB"
Mar 13 19:53:55 minikube cri-dockerd[1756]: time="2025-03-13T19:53:55Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 302e3ee49805: Downloading [============>                                      ]  7.408MB/29.13MB"
Mar 13 19:54:05 minikube cri-dockerd[1756]: time="2025-03-13T19:54:05Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 302e3ee49805: Downloading [===============>                                   ]  8.891MB/29.13MB"
Mar 13 19:54:15 minikube cri-dockerd[1756]: time="2025-03-13T19:54:15Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 835ec61f74e2: Downloading [============================>                      ]  1.175MB/2.093MB"
Mar 13 19:54:25 minikube cri-dockerd[1756]: time="2025-03-13T19:54:25Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 643a6ed41aef: Downloading [========>                                          ]  6.658MB/38.18MB"
Mar 13 19:54:35 minikube cri-dockerd[1756]: time="2025-03-13T19:54:35Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 302e3ee49805: Downloading [====================>                              ]  11.85MB/29.13MB"
Mar 13 19:54:45 minikube cri-dockerd[1756]: time="2025-03-13T19:54:45Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 302e3ee49805: Downloading [=====================>                             ]  12.45MB/29.13MB"
Mar 13 19:54:55 minikube cri-dockerd[1756]: time="2025-03-13T19:54:55Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 302e3ee49805: Downloading [======================>                            ]  13.04MB/29.13MB"
Mar 13 19:55:05 minikube cri-dockerd[1756]: time="2025-03-13T19:55:05Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 302e3ee49805: Downloading [=======================>                           ]  13.63MB/29.13MB"
Mar 13 19:55:15 minikube cri-dockerd[1756]: time="2025-03-13T19:55:15Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 643a6ed41aef: Downloading [==============>                                    ]  11.37MB/38.18MB"
Mar 13 19:55:25 minikube cri-dockerd[1756]: time="2025-03-13T19:55:25Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 643a6ed41aef: Downloading [==================>                                ]  14.12MB/38.18MB"
Mar 13 19:55:35 minikube cri-dockerd[1756]: time="2025-03-13T19:55:35Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 302e3ee49805: Downloading [=====================================>             ]  21.92MB/29.13MB"
Mar 13 19:55:45 minikube cri-dockerd[1756]: time="2025-03-13T19:55:45Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 643a6ed41aef: Downloading [=======================>                           ]  17.64MB/38.18MB"
Mar 13 19:55:55 minikube cri-dockerd[1756]: time="2025-03-13T19:55:55Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 302e3ee49805: Downloading [==========================================>        ]  24.58MB/29.13MB"
Mar 13 19:56:05 minikube cri-dockerd[1756]: time="2025-03-13T19:56:05Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 302e3ee49805: Downloading [============================================>      ]  26.07MB/29.13MB"
Mar 13 19:56:15 minikube cri-dockerd[1756]: time="2025-03-13T19:56:15Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 302e3ee49805: Downloading [===============================================>   ]  27.55MB/29.13MB"
Mar 13 19:56:25 minikube cri-dockerd[1756]: time="2025-03-13T19:56:25Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 302e3ee49805: Downloading [=================================================> ]  28.73MB/29.13MB"
Mar 13 19:56:35 minikube cri-dockerd[1756]: time="2025-03-13T19:56:35Z" level=info msg="Pulling image dockersamples/examplevotingapp_result:before: 643a6ed41aef: Downloading [===============================>                   ]  23.89MB/38.18MB"


==> container status <==
CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
bc60c56c0151c       redis@sha256:02419de7eddf55aa5bcf49efb74e88fa8d931b4d77c07eff8a6b2144472b6952   3 minutes ago       Running             redis                     0                   9520191ba2480       redis-b54754468-zkjs2
d8de3041ed1f8       6e38f40d628db                                                                   8 minutes ago       Running             storage-provisioner       0                   d0b61a35b52c8       storage-provisioner
f3d03b779cb88       c69fa2e9cbf5f                                                                   8 minutes ago       Running             coredns                   0                   db3852b36d3f4       coredns-668d6bf9bc-b4ppq
2f241157d85ab       c69fa2e9cbf5f                                                                   8 minutes ago       Running             coredns                   0                   97813a7368412       coredns-668d6bf9bc-zvfwg
e4f70be46cde1       040f9f8aac8cd                                                                   8 minutes ago       Running             kube-proxy                0                   a35980f719792       kube-proxy-984fw
8917e77d473bc       a389e107f4ff1                                                                   8 minutes ago       Running             kube-scheduler            0                   e997ed9cb01aa       kube-scheduler-minikube
6eb7db2a6c663       c2e17b8d0f4a3                                                                   8 minutes ago       Running             kube-apiserver            0                   b4fe030f0f087       kube-apiserver-minikube
f92831079c6dc       a9e7e6b294baf                                                                   8 minutes ago       Running             etcd                      0                   a64d1264bbb43       etcd-minikube
6e220e0a4a31a       8cab3d2a8bd0f                                                                   8 minutes ago       Running             kube-controller-manager   0                   540e64cbd0bf3       kube-controller-manager-minikube


==> coredns [2f241157d85a] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9


==> coredns [f3d03b779cb8] <==
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_03_13T20_48_34_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Thu, 13 Mar 2025 19:48:30 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Thu, 13 Mar 2025 19:56:33 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Thu, 13 Mar 2025 19:53:09 +0000   Thu, 13 Mar 2025 19:48:29 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Thu, 13 Mar 2025 19:53:09 +0000   Thu, 13 Mar 2025 19:48:29 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Thu, 13 Mar 2025 19:53:09 +0000   Thu, 13 Mar 2025 19:48:29 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Thu, 13 Mar 2025 19:53:09 +0000   Thu, 13 Mar 2025 19:48:31 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8054808Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             8054808Ki
  pods:               110
System Info:
  Machine ID:                 7ef4a01ff7af417f850c21cf26376d19
  System UUID:                7ef4a01ff7af417f850c21cf26376d19
  Boot ID:                    09f66b45-29f9-488c-95a6-81004872c040
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (13 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-668d6bf9bc-b4ppq            100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     8m4s
  kube-system                 coredns-668d6bf9bc-zvfwg            100m (1%)     0 (0%)      70Mi (0%)        170Mi (2%)     8m4s
  kube-system                 etcd-minikube                       100m (1%)     0 (0%)      100Mi (1%)       0 (0%)         8m11s
  kube-system                 kube-apiserver-minikube             250m (3%)     0 (0%)      0 (0%)           0 (0%)         8m11s
  kube-system                 kube-controller-manager-minikube    200m (2%)     0 (0%)      0 (0%)           0 (0%)         8m8s
  kube-system                 kube-proxy-984fw                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m4s
  kube-system                 kube-scheduler-minikube             100m (1%)     0 (0%)      0 (0%)           0 (0%)         8m9s
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m7s
  vote-kubernetes             db-84884c468f-rw2wx                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m1s
  vote-kubernetes             redis-b54754468-zkjs2               0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m
  vote-kubernetes             result-7dd8bf8b6f-s4gjq             0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m
  vote-kubernetes             vote-79b68cc66-d7x5g                0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m
  vote-kubernetes             worker-6f5f6cdd56-xxbv9             0 (0%)        0 (0%)      0 (0%)           0 (0%)         8m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (10%)  0 (0%)
  memory             240Mi (3%)  340Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                    From             Message
  ----     ------                             ----                   ----             -------
  Normal   Starting                           8m2s                   kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  8m16s                  kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           8m16s                  kubelet          Starting kubelet.
  Warning  CgroupV1                           8m16s                  kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            8m16s (x8 over 8m16s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              8m16s (x8 over 8m16s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               8m16s (x7 over 8m16s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            8m16s                  kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  8m9s                   kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           8m9s                   kubelet          Starting kubelet.
  Warning  CgroupV1                           8m9s                   kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            8m9s                   kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            8m9s                   kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              8m9s                   kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               8m9s                   kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     8m5s                   node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[Mar13 17:52] MDS CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/mds.html for more details.
[  +0.000000] TAA CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/tsx_async_abort.html for more details.
[  +0.000000] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]  #2 #3 #4 #5 #6 #7
[  +0.033156] PCI: Fatal: No config space access function found
[  +0.026855] PCI: System does not support PCI
[  +0.135954] kvm: already loaded the other module
[  +2.346329] FS-Cache: Duplicate cookie detected
[  +0.003364] FS-Cache: O-cookie c=00000006 [p=00000002 fl=222 nc=0 na=1]
[  +0.005431] FS-Cache: O-cookie d=00000000f1eb7f17{9P.session} n=0000000017e3a298
[  +0.002476] FS-Cache: O-key=[10] '34323934393337353531'
[  +0.000952] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.003410] FS-Cache: N-cookie d=00000000f1eb7f17{9P.session} n=000000001a62f7d7
[  +0.001185] FS-Cache: N-key=[10] '34323934393337353531'
[  +0.657490] FS-Cache: Duplicate cookie detected
[  +0.002887] FS-Cache: O-cookie c=00000008 [p=00000002 fl=222 nc=0 na=1]
[  +0.000843] FS-Cache: O-cookie d=00000000f1eb7f17{9P.session} n=000000001615c3ea
[  +0.000904] FS-Cache: O-key=[10] '34323934393337363138'
[  +0.000630] FS-Cache: N-cookie c=00000009 [p=00000002 fl=2 nc=0 na=1]
[  +0.000786] FS-Cache: N-cookie d=00000000f1eb7f17{9P.session} n=000000004b18af50
[  +0.001211] FS-Cache: N-key=[10] '34323934393337363138'
[  +0.056796] WSL (1) ERROR: ConfigApplyWindowsLibPath:2531: open /etc/ld.so.conf.d/ld.wsl.conf
[  +0.000009]  failed 2
[  +0.038360] WSL (1) WARNING: /usr/share/zoneinfo/Europe/Paris not found. Is the tzdata package installed?
[  +0.540231] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.025091] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001415] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001170] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001416] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +1.667498] netlink: 'init': attribute type 4 has an invalid length.
[Mar13 17:57] tmpfs: Unknown parameter 'noswap'
[  +6.833818] tmpfs: Unknown parameter 'noswap'
[Mar13 19:44] tmpfs: Unknown parameter 'noswap'
[  +6.097663] tmpfs: Unknown parameter 'noswap'
[Mar13 19:48] tmpfs: Unknown parameter 'noswap'
[  +6.971359] tmpfs: Unknown parameter 'noswap'


==> etcd [f92831079c6d] <==
{"level":"warn","ts":"2025-03-13T19:48:28.173093Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-03-13T19:48:28.173487Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-03-13T19:48:28.173624Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-03-13T19:48:28.173666Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-03-13T19:48:28.173720Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-13T19:48:28.174697Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-03-13T19:48:28.174907Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":8,"max-cpu-available":8,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-03-13T19:48:28.182179Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"6.077939ms"}
{"level":"info","ts":"2025-03-13T19:48:28.191595Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-03-13T19:48:28.191844Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-03-13T19:48:28.191921Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-03-13T19:48:28.191940Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-03-13T19:48:28.191948Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-03-13T19:48:28.192056Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-03-13T19:48:28.266279Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-03-13T19:48:28.269421Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-03-13T19:48:28.273637Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-03-13T19:48:28.276809Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-03-13T19:48:28.277606Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-03-13T19:48:28.277668Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-13T19:48:28.277776Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-13T19:48:28.277890Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-13T19:48:28.277913Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-13T19:48:28.280506Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-03-13T19:48:28.281066Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-03-13T19:48:28.281301Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-13T19:48:28.281506Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-13T19:48:28.281543Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-03-13T19:48:28.281666Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-03-13T19:48:28.281715Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-03-13T19:48:28.493263Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-03-13T19:48:28.493341Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-03-13T19:48:28.493389Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-03-13T19:48:28.493414Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-03-13T19:48:28.493423Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-03-13T19:48:28.493435Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-03-13T19:48:28.493446Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-03-13T19:48:28.495411Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-13T19:48:28.496804Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-03-13T19:48:28.496834Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-13T19:48:28.496889Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-13T19:48:28.497155Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-03-13T19:48:28.497182Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-03-13T19:48:28.498138Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-13T19:48:28.498897Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-13T19:48:28.499169Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-03-13T19:48:28.500574Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-03-13T19:48:28.500708Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-13T19:48:28.501250Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-13T19:48:28.501415Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}


==> kernel <==
 19:56:42 up  2:04,  0 users,  load average: 0.62, 0.49, 0.45
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [6eb7db2a6c66] <==
I0313 19:48:30.775892       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0313 19:48:30.776304       1 aggregator.go:169] waiting for initial CRD sync...
I0313 19:48:30.776340       1 controller.go:78] Starting OpenAPI AggregationController
I0313 19:48:30.776425       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0313 19:48:30.776496       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0313 19:48:30.834683       1 remote_available_controller.go:411] Starting RemoteAvailability controller
I0313 19:48:30.835273       1 cache.go:32] Waiting for caches to sync for RemoteAvailability controller
I0313 19:48:30.835505       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0313 19:48:30.835706       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0313 19:48:30.834679       1 controller.go:142] Starting OpenAPI controller
I0313 19:48:30.835432       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0313 19:48:30.835447       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0313 19:48:30.834720       1 controller.go:90] Starting OpenAPI V3 controller
I0313 19:48:30.834737       1 naming_controller.go:294] Starting NamingConditionController
I0313 19:48:30.834760       1 establishing_controller.go:81] Starting EstablishingController
I0313 19:48:30.834774       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0313 19:48:30.834793       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0313 19:48:30.834807       1 crd_finalizer.go:269] Starting CRDFinalizer
I0313 19:48:30.770074       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0313 19:48:30.834825       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0313 19:48:30.838213       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0313 19:48:30.877843       1 shared_informer.go:320] Caches are synced for configmaps
I0313 19:48:30.878676       1 cache.go:39] Caches are synced for LocalAvailability controller
I0313 19:48:30.878920       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0313 19:48:30.878939       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0313 19:48:30.892570       1 shared_informer.go:320] Caches are synced for node_authorizer
I0313 19:48:30.902023       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0313 19:48:30.902071       1 policy_source.go:240] refreshing policies
E0313 19:48:30.920532       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
E0313 19:48:30.932128       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0313 19:48:30.936937       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0313 19:48:30.936989       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0313 19:48:30.937083       1 aggregator.go:171] initial CRD sync complete...
I0313 19:48:30.937102       1 autoregister_controller.go:144] Starting autoregister controller
I0313 19:48:30.937118       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0313 19:48:30.937131       1 cache.go:39] Caches are synced for autoregister controller
I0313 19:48:30.938314       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0313 19:48:30.938378       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0313 19:48:30.938520       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0313 19:48:30.971714       1 controller.go:615] quota admission added evaluator for: namespaces
I0313 19:48:31.122855       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0313 19:48:31.776656       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0313 19:48:31.781121       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0313 19:48:31.781152       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0313 19:48:32.471440       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0313 19:48:32.526643       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0313 19:48:32.680529       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0313 19:48:32.688773       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [192.168.49.2]
I0313 19:48:32.689880       1 controller.go:615] quota admission added evaluator for: endpoints
I0313 19:48:32.696912       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0313 19:48:32.861645       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0313 19:48:33.699221       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0313 19:48:33.772915       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0313 19:48:33.786445       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0313 19:48:38.160270       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0313 19:48:38.412697       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0313 19:48:41.968303       1 alloc.go:330] "allocated clusterIPs" service="vote-kubernetes/db" clusterIPs={"IPv4":"10.99.151.95"}
I0313 19:48:42.130600       1 alloc.go:330] "allocated clusterIPs" service="vote-kubernetes/redis" clusterIPs={"IPv4":"10.107.205.6"}
I0313 19:48:42.232756       1 alloc.go:330] "allocated clusterIPs" service="vote-kubernetes/result" clusterIPs={"IPv4":"10.105.219.28"}
I0313 19:48:42.333744       1 alloc.go:330] "allocated clusterIPs" service="vote-kubernetes/vote" clusterIPs={"IPv4":"10.106.101.14"}


==> kube-controller-manager [6e220e0a4a31] <==
I0313 19:48:37.412517       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0313 19:48:37.412557       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0313 19:48:37.412606       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0313 19:48:37.412608       1 shared_informer.go:320] Caches are synced for stateful set
I0313 19:48:37.412625       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0313 19:48:37.412860       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0313 19:48:37.415419       1 shared_informer.go:320] Caches are synced for namespace
I0313 19:48:37.416656       1 shared_informer.go:320] Caches are synced for GC
I0313 19:48:37.418369       1 shared_informer.go:320] Caches are synced for node
I0313 19:48:37.418456       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0313 19:48:37.418482       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0313 19:48:37.418489       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0313 19:48:37.418498       1 shared_informer.go:320] Caches are synced for cidrallocator
I0313 19:48:37.418999       1 shared_informer.go:320] Caches are synced for service account
I0313 19:48:37.423257       1 shared_informer.go:320] Caches are synced for resource quota
I0313 19:48:37.425357       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0313 19:48:37.425411       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0313 19:48:37.425612       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0313 19:48:37.426574       1 shared_informer.go:320] Caches are synced for persistent volume
I0313 19:48:37.430017       1 shared_informer.go:320] Caches are synced for garbage collector
I0313 19:48:37.436354       1 shared_informer.go:320] Caches are synced for deployment
I0313 19:48:38.117600       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0313 19:48:38.541038       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="123.20204ms"
I0313 19:48:38.551048       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="9.960055ms"
I0313 19:48:38.551184       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="56.904Âµs"
I0313 19:48:38.554883       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="73.906Âµs"
I0313 19:48:38.569549       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="57.104Âµs"
I0313 19:48:40.132124       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="50.504Âµs"
I0313 19:48:40.158525       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="69.205Âµs"
I0313 19:48:41.035585       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="9.331708ms"
I0313 19:48:41.035693       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="36.703Âµs"
I0313 19:48:41.963068       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/db-84884c468f" duration="26.079077ms"
I0313 19:48:41.985912       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/db-84884c468f" duration="22.527908ms"
I0313 19:48:41.986042       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/db-84884c468f" duration="54.904Âµs"
E0313 19:48:42.084686       1 pv_controller.go:1587] "Error finding provisioning plugin for claim" err="storageclass.storage.k8s.io \"manual\" not found" logger="persistentvolume-binder-controller" PVC="vote-kubernetes/db-volume-claim"
I0313 19:48:42.129848       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/redis-b54754468" duration="21.309116ms"
I0313 19:48:42.146283       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/redis-b54754468" duration="16.171426ms"
I0313 19:48:42.146589       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/redis-b54754468" duration="146.411Âµs"
I0313 19:48:42.181497       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/redis-b54754468" duration="59.904Âµs"
I0313 19:48:42.205191       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/result-7dd8bf8b6f" duration="17.976863ms"
I0313 19:48:42.225175       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/result-7dd8bf8b6f" duration="19.834004ms"
I0313 19:48:42.225403       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/result-7dd8bf8b6f" duration="47.704Âµs"
I0313 19:48:42.225610       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/result-7dd8bf8b6f" duration="30.402Âµs"
I0313 19:48:42.312418       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/vote-79b68cc66" duration="18.288186ms"
I0313 19:48:42.324265       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/vote-79b68cc66" duration="11.760791ms"
I0313 19:48:42.324496       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/vote-79b68cc66" duration="123.309Âµs"
I0313 19:48:42.332792       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/vote-79b68cc66" duration="52.704Âµs"
I0313 19:48:42.415736       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/worker-6f5f6cdd56" duration="28.780982ms"
I0313 19:48:42.429846       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/worker-6f5f6cdd56" duration="14.049665ms"
I0313 19:48:42.429987       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/worker-6f5f6cdd56" duration="61.705Âµs"
I0313 19:48:42.431902       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/worker-6f5f6cdd56" duration="50.104Âµs"
I0313 19:48:43.403793       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/db-84884c468f" duration="51.204Âµs"
I0313 19:48:44.300540       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="7.24435ms"
I0313 19:48:44.300621       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="46.103Âµs"
I0313 19:48:44.350188       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0313 19:48:57.409482       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/db-84884c468f" duration="71.605Âµs"
I0313 19:48:57.423917       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/db-84884c468f" duration="50.104Âµs"
I0313 19:53:01.400303       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/redis-b54754468" duration="9.326786ms"
I0313 19:53:01.400469       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="vote-kubernetes/redis-b54754468" duration="41.703Âµs"
I0313 19:53:09.080439       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [e4f70be46cde] <==
I0313 19:48:38.886541       1 server_linux.go:66] "Using iptables proxy"
I0313 19:48:39.118487       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0313 19:48:39.118636       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0313 19:48:39.187513       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0313 19:48:39.187611       1 server_linux.go:170] "Using iptables Proxier"
I0313 19:48:39.191231       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0313 19:48:39.201835       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0313 19:48:39.217681       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0313 19:48:39.218507       1 server.go:497] "Version info" version="v1.32.0"
I0313 19:48:39.218566       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0313 19:48:39.227812       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0313 19:48:39.237022       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0313 19:48:39.239244       1 config.go:199] "Starting service config controller"
I0313 19:48:39.239336       1 config.go:329] "Starting node config controller"
I0313 19:48:39.239349       1 shared_informer.go:313] Waiting for caches to sync for service config
I0313 19:48:39.239351       1 shared_informer.go:313] Waiting for caches to sync for node config
I0313 19:48:39.239463       1 config.go:105] "Starting endpoint slice config controller"
I0313 19:48:39.239497       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0313 19:48:39.340413       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0313 19:48:39.340469       1 shared_informer.go:320] Caches are synced for service config
I0313 19:48:39.340485       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [8917e77d473b] <==
I0313 19:48:30.886588       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0313 19:48:30.888426       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0313 19:48:30.888465       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0313 19:48:30.888475       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0313 19:48:30.888543       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0313 19:48:30.890035       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0313 19:48:30.890092       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0313 19:48:30.891748       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0313 19:48:30.891825       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0313 19:48:30.890352       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0313 19:48:30.892042       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0313 19:48:30.892542       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0313 19:48:30.892618       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0313 19:48:30.892925       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0313 19:48:30.892969       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0313 19:48:30.893026       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0313 19:48:30.893043       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0313 19:48:30.893092       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0313 19:48:30.893135       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0313 19:48:30.893337       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0313 19:48:30.893382       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0313 19:48:30.893435       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0313 19:48:30.893513       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0313 19:48:30.894240       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0313 19:48:30.894258       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0313 19:48:30.894287       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0313 19:48:30.894313       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0313 19:48:30.894391       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0313 19:48:30.894411       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0313 19:48:30.894567       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0313 19:48:30.894649       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0313 19:48:30.894666       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0313 19:48:30.894678       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0313 19:48:30.894703       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0313 19:48:30.894672       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0313 19:48:30.895160       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0313 19:48:30.895201       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0313 19:48:31.849156       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0313 19:48:31.849209       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0313 19:48:31.859101       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0313 19:48:31.859155       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0313 19:48:31.893004       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0313 19:48:31.893046       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0313 19:48:31.913728       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0313 19:48:31.913766       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0313 19:48:31.916982       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0313 19:48:31.917031       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0313 19:48:31.925669       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0313 19:48:31.925714       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0313 19:48:32.001409       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0313 19:48:32.001455       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0313 19:48:32.073799       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0313 19:48:32.073877       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0313 19:48:32.166980       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0313 19:48:32.167010       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0313 19:48:32.182629       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0313 19:48:32.182688       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0313 19:48:32.244832       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0313 19:48:32.244878       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I0313 19:48:34.389081       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Mar 13 19:48:34 minikube kubelet[2609]: E0313 19:48:34.099913    2609 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: E0313 19:48:34.100639    2609 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: E0313 19:48:34.100641    2609 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.106786    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/2b4b75c2a289008e0b381891e9683040-etcd-data\") pod \"etcd-minikube\" (UID: \"2b4b75c2a289008e0b381891e9683040\") " pod="kube-system/etcd-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.106873    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.106908    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.106951    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.106977    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.107003    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.107029    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.107107    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.107143    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/d14ce008bee3a1f3bd7cf547688f9dfe-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"d14ce008bee3a1f3bd7cf547688f9dfe\") " pod="kube-system/kube-scheduler-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.107178    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/d72d0a4cf4be077c9919d46b7358a5e8-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"d72d0a4cf4be077c9919d46b7358a5e8\") " pod="kube-system/kube-apiserver-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.107225    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.107268    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.107329    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.107363    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/2b4b75c2a289008e0b381891e9683040-etcd-certs\") pod \"etcd-minikube\" (UID: \"2b4b75c2a289008e0b381891e9683040\") " pod="kube-system/etcd-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.107396    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.694719    2609 apiserver.go:52] "Watching apiserver"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.704374    2609 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.754866    2609 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=1.7548238 podStartE2EDuration="1.7548238s" podCreationTimestamp="2025-03-13 19:48:33 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-13 19:48:34.754765896 +0000 UTC m=+1.143123547" watchObservedRunningTime="2025-03-13 19:48:34.7548238 +0000 UTC m=+1.143181451"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.787103    2609 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=3.787044103 podStartE2EDuration="3.787044103s" podCreationTimestamp="2025-03-13 19:48:31 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-13 19:48:34.771722861 +0000 UTC m=+1.160080512" watchObservedRunningTime="2025-03-13 19:48:34.787044103 +0000 UTC m=+1.175401854"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.800091    2609 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=3.800067674 podStartE2EDuration="3.800067674s" podCreationTimestamp="2025-03-13 19:48:31 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-13 19:48:34.787441633 +0000 UTC m=+1.175799284" watchObservedRunningTime="2025-03-13 19:48:34.800067674 +0000 UTC m=+1.188425325"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.800280    2609 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=0.800265689 podStartE2EDuration="800.265689ms" podCreationTimestamp="2025-03-13 19:48:34 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-13 19:48:34.799790254 +0000 UTC m=+1.188147905" watchObservedRunningTime="2025-03-13 19:48:34.800265689 +0000 UTC m=+1.188623340"
Mar 13 19:48:34 minikube kubelet[2609]: I0313 19:48:34.889858    2609 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Mar 13 19:48:34 minikube kubelet[2609]: E0313 19:48:34.898954    2609 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Mar 13 19:48:37 minikube kubelet[2609]: I0313 19:48:37.435495    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/03f538be-c69e-40ad-baf0-61231ba3f5f4-tmp\") pod \"storage-provisioner\" (UID: \"03f538be-c69e-40ad-baf0-61231ba3f5f4\") " pod="kube-system/storage-provisioner"
Mar 13 19:48:37 minikube kubelet[2609]: I0313 19:48:37.435550    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-sdxkf\" (UniqueName: \"kubernetes.io/projected/03f538be-c69e-40ad-baf0-61231ba3f5f4-kube-api-access-sdxkf\") pod \"storage-provisioner\" (UID: \"03f538be-c69e-40ad-baf0-61231ba3f5f4\") " pod="kube-system/storage-provisioner"
Mar 13 19:48:37 minikube kubelet[2609]: E0313 19:48:37.541101    2609 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Mar 13 19:48:37 minikube kubelet[2609]: E0313 19:48:37.541146    2609 projected.go:194] Error preparing data for projected volume kube-api-access-sdxkf for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Mar 13 19:48:37 minikube kubelet[2609]: E0313 19:48:37.541231    2609 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/03f538be-c69e-40ad-baf0-61231ba3f5f4-kube-api-access-sdxkf podName:03f538be-c69e-40ad-baf0-61231ba3f5f4 nodeName:}" failed. No retries permitted until 2025-03-13 19:48:38.041199889 +0000 UTC m=+4.429557540 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-sdxkf" (UniqueName: "kubernetes.io/projected/03f538be-c69e-40ad-baf0-61231ba3f5f4-kube-api-access-sdxkf") pod "storage-provisioner" (UID: "03f538be-c69e-40ad-baf0-61231ba3f5f4") : configmap "kube-root-ca.crt" not found
Mar 13 19:48:38 minikube kubelet[2609]: E0313 19:48:38.041326    2609 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Mar 13 19:48:38 minikube kubelet[2609]: E0313 19:48:38.041374    2609 projected.go:194] Error preparing data for projected volume kube-api-access-sdxkf for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Mar 13 19:48:38 minikube kubelet[2609]: E0313 19:48:38.041431    2609 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/03f538be-c69e-40ad-baf0-61231ba3f5f4-kube-api-access-sdxkf podName:03f538be-c69e-40ad-baf0-61231ba3f5f4 nodeName:}" failed. No retries permitted until 2025-03-13 19:48:39.041417791 +0000 UTC m=+5.429775542 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-sdxkf" (UniqueName: "kubernetes.io/projected/03f538be-c69e-40ad-baf0-61231ba3f5f4-kube-api-access-sdxkf") pod "storage-provisioner" (UID: "03f538be-c69e-40ad-baf0-61231ba3f5f4") : configmap "kube-root-ca.crt" not found
Mar 13 19:48:38 minikube kubelet[2609]: I0313 19:48:38.243322    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/549193fe-5de3-4e50-9a30-08482a128b13-kube-proxy\") pod \"kube-proxy-984fw\" (UID: \"549193fe-5de3-4e50-9a30-08482a128b13\") " pod="kube-system/kube-proxy-984fw"
Mar 13 19:48:38 minikube kubelet[2609]: I0313 19:48:38.243418    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/549193fe-5de3-4e50-9a30-08482a128b13-lib-modules\") pod \"kube-proxy-984fw\" (UID: \"549193fe-5de3-4e50-9a30-08482a128b13\") " pod="kube-system/kube-proxy-984fw"
Mar 13 19:48:38 minikube kubelet[2609]: I0313 19:48:38.243443    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-tdzxh\" (UniqueName: \"kubernetes.io/projected/549193fe-5de3-4e50-9a30-08482a128b13-kube-api-access-tdzxh\") pod \"kube-proxy-984fw\" (UID: \"549193fe-5de3-4e50-9a30-08482a128b13\") " pod="kube-system/kube-proxy-984fw"
Mar 13 19:48:38 minikube kubelet[2609]: I0313 19:48:38.243466    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/549193fe-5de3-4e50-9a30-08482a128b13-xtables-lock\") pod \"kube-proxy-984fw\" (UID: \"549193fe-5de3-4e50-9a30-08482a128b13\") " pod="kube-system/kube-proxy-984fw"
Mar 13 19:48:38 minikube kubelet[2609]: I0313 19:48:38.546079    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-fjhcz\" (UniqueName: \"kubernetes.io/projected/ca6e76f6-534c-45d9-be98-9b06f92e99f1-kube-api-access-fjhcz\") pod \"coredns-668d6bf9bc-zvfwg\" (UID: \"ca6e76f6-534c-45d9-be98-9b06f92e99f1\") " pod="kube-system/coredns-668d6bf9bc-zvfwg"
Mar 13 19:48:38 minikube kubelet[2609]: I0313 19:48:38.546175    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/ca6e76f6-534c-45d9-be98-9b06f92e99f1-config-volume\") pod \"coredns-668d6bf9bc-zvfwg\" (UID: \"ca6e76f6-534c-45d9-be98-9b06f92e99f1\") " pod="kube-system/coredns-668d6bf9bc-zvfwg"
Mar 13 19:48:38 minikube kubelet[2609]: I0313 19:48:38.647320    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/c0c95f61-9693-479e-a411-a20a64bdc7c8-config-volume\") pod \"coredns-668d6bf9bc-b4ppq\" (UID: \"c0c95f61-9693-479e-a411-a20a64bdc7c8\") " pod="kube-system/coredns-668d6bf9bc-b4ppq"
Mar 13 19:48:38 minikube kubelet[2609]: I0313 19:48:38.647387    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-nfvhx\" (UniqueName: \"kubernetes.io/projected/c0c95f61-9693-479e-a411-a20a64bdc7c8-kube-api-access-nfvhx\") pod \"coredns-668d6bf9bc-b4ppq\" (UID: \"c0c95f61-9693-479e-a411-a20a64bdc7c8\") " pod="kube-system/coredns-668d6bf9bc-b4ppq"
Mar 13 19:48:39 minikube kubelet[2609]: I0313 19:48:39.090451    2609 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="97813a73684121912ab35cfcf31c8360f71a8a07be517d1dd9c99ee73abedc5b"
Mar 13 19:48:39 minikube kubelet[2609]: I0313 19:48:39.168910    2609 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-984fw" podStartSLOduration=1.168891264 podStartE2EDuration="1.168891264s" podCreationTimestamp="2025-03-13 19:48:38 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-13 19:48:39.168765455 +0000 UTC m=+5.557123106" watchObservedRunningTime="2025-03-13 19:48:39.168891264 +0000 UTC m=+5.557248915"
Mar 13 19:48:40 minikube kubelet[2609]: I0313 19:48:40.145821    2609 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-668d6bf9bc-zvfwg" podStartSLOduration=2.145797123 podStartE2EDuration="2.145797123s" podCreationTimestamp="2025-03-13 19:48:38 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-13 19:48:40.132334702 +0000 UTC m=+6.520692353" watchObservedRunningTime="2025-03-13 19:48:40.145797123 +0000 UTC m=+6.534154874"
Mar 13 19:48:40 minikube kubelet[2609]: I0313 19:48:40.158714    2609 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=5.1586877 podStartE2EDuration="5.1586877s" podCreationTimestamp="2025-03-13 19:48:35 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-13 19:48:40.145923932 +0000 UTC m=+6.534281583" watchObservedRunningTime="2025-03-13 19:48:40.1586877 +0000 UTC m=+6.547045451"
Mar 13 19:48:40 minikube kubelet[2609]: I0313 19:48:40.158986    2609 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-668d6bf9bc-b4ppq" podStartSLOduration=2.158970621 podStartE2EDuration="2.158970621s" podCreationTimestamp="2025-03-13 19:48:38 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-13 19:48:40.158318272 +0000 UTC m=+6.546675923" watchObservedRunningTime="2025-03-13 19:48:40.158970621 +0000 UTC m=+6.547328272"
Mar 13 19:48:41 minikube kubelet[2609]: I0313 19:48:41.145656    2609 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Mar 13 19:48:42 minikube kubelet[2609]: I0313 19:48:42.297209    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-jtlh6\" (UniqueName: \"kubernetes.io/projected/d8fe539a-fc52-4435-82f5-766c97df0415-kube-api-access-jtlh6\") pod \"redis-b54754468-zkjs2\" (UID: \"d8fe539a-fc52-4435-82f5-766c97df0415\") " pod="vote-kubernetes/redis-b54754468-zkjs2"
Mar 13 19:48:42 minikube kubelet[2609]: I0313 19:48:42.297328    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"redis-storage\" (UniqueName: \"kubernetes.io/empty-dir/d8fe539a-fc52-4435-82f5-766c97df0415-redis-storage\") pod \"redis-b54754468-zkjs2\" (UID: \"d8fe539a-fc52-4435-82f5-766c97df0415\") " pod="vote-kubernetes/redis-b54754468-zkjs2"
Mar 13 19:48:42 minikube kubelet[2609]: I0313 19:48:42.297359    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-qtgfv\" (UniqueName: \"kubernetes.io/projected/a84b8779-fb11-44c2-b2e7-48e3886f4fd3-kube-api-access-qtgfv\") pod \"result-7dd8bf8b6f-s4gjq\" (UID: \"a84b8779-fb11-44c2-b2e7-48e3886f4fd3\") " pod="vote-kubernetes/result-7dd8bf8b6f-s4gjq"
Mar 13 19:48:42 minikube kubelet[2609]: I0313 19:48:42.499032    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-k86b4\" (UniqueName: \"kubernetes.io/projected/8fe16d39-c682-47ac-8fda-bf7c7251f868-kube-api-access-k86b4\") pod \"vote-79b68cc66-d7x5g\" (UID: \"8fe16d39-c682-47ac-8fda-bf7c7251f868\") " pod="vote-kubernetes/vote-79b68cc66-d7x5g"
Mar 13 19:48:42 minikube kubelet[2609]: I0313 19:48:42.499092    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-v2d8d\" (UniqueName: \"kubernetes.io/projected/b501a058-288f-42cf-9a75-f8eb5e40d4ba-kube-api-access-v2d8d\") pod \"worker-6f5f6cdd56-xxbv9\" (UID: \"b501a058-288f-42cf-9a75-f8eb5e40d4ba\") " pod="vote-kubernetes/worker-6f5f6cdd56-xxbv9"
Mar 13 19:48:43 minikube kubelet[2609]: I0313 19:48:43.226573    2609 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="e649ae3c66d3cd47555d169eb4c8ce1b251576a6e53920a45c910c924bc6a6c2"
Mar 13 19:48:44 minikube kubelet[2609]: I0313 19:48:44.282978    2609 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Mar 13 19:48:44 minikube kubelet[2609]: I0313 19:48:44.338732    2609 kuberuntime_manager.go:1702] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Mar 13 19:48:44 minikube kubelet[2609]: I0313 19:48:44.339483    2609 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
Mar 13 19:48:57 minikube kubelet[2609]: I0313 19:48:57.603170    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"db-volume\" (UniqueName: \"kubernetes.io/host-path/8ee8833f-8b7f-4b06-abf2-1d12657c2ae7-db-volume\") pod \"db-84884c468f-rw2wx\" (UID: \"8ee8833f-8b7f-4b06-abf2-1d12657c2ae7\") " pod="vote-kubernetes/db-84884c468f-rw2wx"
Mar 13 19:48:57 minikube kubelet[2609]: I0313 19:48:57.603272    2609 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-dtnk5\" (UniqueName: \"kubernetes.io/projected/8ee8833f-8b7f-4b06-abf2-1d12657c2ae7-kube-api-access-dtnk5\") pod \"db-84884c468f-rw2wx\" (UID: \"8ee8833f-8b7f-4b06-abf2-1d12657c2ae7\") " pod="vote-kubernetes/db-84884c468f-rw2wx"
Mar 13 19:53:01 minikube kubelet[2609]: I0313 19:53:01.391062    2609 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="vote-kubernetes/redis-b54754468-zkjs2" podStartSLOduration=1.6209978889999999 podStartE2EDuration="4m19.39087054s" podCreationTimestamp="2025-03-13 19:48:42 +0000 UTC" firstStartedPulling="2025-03-13 19:48:43.067938848 +0000 UTC m=+9.456296499" lastFinishedPulling="2025-03-13 19:53:00.817910975 +0000 UTC m=+267.226169150" observedRunningTime="2025-03-13 19:53:01.390610921 +0000 UTC m=+267.798869096" watchObservedRunningTime="2025-03-13 19:53:01.39087054 +0000 UTC m=+267.799128715"


==> storage-provisioner [d8de3041ed1f] <==
I0313 19:48:40.003769       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0313 19:48:40.013932       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0313 19:48:40.014009       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0313 19:48:40.021343       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0313 19:48:40.021500       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"60515646-8f7e-49a8-ab12-c11b8d974909", APIVersion:"v1", ResourceVersion:"407", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_2580d96b-fba4-4e7a-ae5e-de651f4712f2 became leader
I0313 19:48:40.021572       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_2580d96b-fba4-4e7a-ae5e-de651f4712f2!
I0313 19:48:40.122592       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_2580d96b-fba4-4e7a-ae5e-de651f4712f2!

